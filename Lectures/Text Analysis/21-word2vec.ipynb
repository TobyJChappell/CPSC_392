{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduced in 2013, word2vec is an unsupervised neural network approach to representing words in a document. Given a text corpus, this algorithm can output a vector representation of each word.\n",
    "\n",
    "##### It uses the context (surrounding words) of a word to generate its representation.\n",
    "\n",
    "\n",
    "##### We start by loading the gensim library, which is a famous library for topic modelling and NLP applications in Python. I have already downloaded the Gutenberg data, which includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/tobychappell/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tobychappell/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk; nltk.download(\"gutenberg\")\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting sentences from the data and running the word2vec model on it. It takes a long time so I have saved the model and will load it for future use. Notice how easy it is to run this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = gutenberg.sents()\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n",
    "model.save('gutenberg_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I have saved the model as gutenberg_model, now we can load it and look at vector representation of different words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('gutenberg_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By default, the model is made to generate a 100 value long vector for each word. This is the output of the neural network for each word. The vector representation focuses on the context of the word across all documents, so we can even find what words it is similar to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6397712   0.9403682   0.16476774 -0.003929    0.6514387  -0.2389871\n",
      " -0.32440186  0.03787065  0.13923813 -0.12836191  0.25099906  0.50155777\n",
      " -0.9492648   0.05303491  0.69198805  0.08257672 -0.37582156  0.6737136\n",
      " -0.3205712   0.59403425  1.5550497  -1.3485568   0.62119     0.69504344\n",
      "  0.8442935   0.9219864   0.57053435 -0.06519959 -0.54377174  0.27061206\n",
      " -0.12010787  0.5428703   0.4591472  -0.56457496 -0.09456757 -0.01166055\n",
      "  0.04791519 -0.00829506 -0.6296058  -0.2345837  -0.73390454  0.48389575\n",
      " -0.21756802 -0.0083403  -0.6030183   0.936554    0.79389894  0.5300423\n",
      "  0.15853697 -0.15191227 -0.16740079 -0.89003474  0.8811316   0.1436148\n",
      "  0.22647502  0.6096216   0.4202352  -0.00232328  0.34038994  1.4338999\n",
      " -0.71681017  0.8050907  -0.15280347 -0.1471908  -0.01021012 -0.360948\n",
      "  0.21517235 -0.8977882  -0.86317843 -0.30080226  0.18384501 -0.07696274\n",
      "  0.742148    0.38857204  0.4275953  -1.0878211   0.8870503  -0.15645728\n",
      " -0.34996212  0.9724159  -0.60275555 -0.11190093  0.54653186 -0.47407442\n",
      "  0.02334487  1.1258863  -0.14164384 -0.55071527 -0.27765223  0.6409156\n",
      " -0.1671273   0.12043022 -0.96288276  0.19918746  0.7760684   0.5456088\n",
      "  0.5141872   1.2364513   0.04991437 -0.0176039 ]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['fruit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vine', 0.7773963212966919),\n",
       " ('bread', 0.7728155255317688),\n",
       " ('corn', 0.769756555557251),\n",
       " ('field', 0.7633217573165894),\n",
       " ('blood', 0.7580446004867554),\n",
       " ('meat', 0.7436333298683167),\n",
       " ('skin', 0.742735743522644),\n",
       " ('light', 0.7320427298545837),\n",
       " ('oil', 0.7297345399856567),\n",
       " ('flesh', 0.7287917137145996)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"fruit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice how fruit is similar to vine, corn, bread etc.. given the context of several books in the Gutenberg Project. We can additionally add or subtract the weights of words to see more words come up. In the following example, we want to find words similar to fruit without the context of flesh, meat etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mulberry', 0.7614861726760864),\n",
       " ('forest', 0.7536096572875977),\n",
       " ('plum', 0.7491604089736938),\n",
       " ('salt', 0.7467324733734131),\n",
       " ('vine', 0.7464162707328796),\n",
       " ('flower', 0.7348028421401978),\n",
       " ('fir', 0.7268661260604858),\n",
       " ('desert', 0.7267394065856934),\n",
       " ('wild', 0.7252177000045776),\n",
       " ('soil', 0.7245904207229614)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['fruit',\"corn\"],negative=[\"blood\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here's a model running for a very small data set, 2 sentences with 3 words each. Notice how I can change the size of the output vector using size, and how many words to check in the context, using window. The model isn't good, but that just tells you that text analysis needs lots of data to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "model2 = Word2Vec(sentences, min_count=1,size=3,window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00670908, -0.07993935, -0.13842449], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv['meow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woof', 0.3258308172225952),\n",
       " ('dog', -0.43794459104537964),\n",
       " ('cat', -0.7971633672714233),\n",
       " ('say', -0.7983071208000183)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar(\"meow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
